{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf5cfa6",
   "metadata": {},
   "source": [
    "# Train a binary classifier to classify income as below 50k or above 50k from census data\n",
    "\n",
    "## Dataset Information:\n",
    "\n",
    "The dataset is obtained from [Adult Data Set](https://archive.ics.uci.edu/ml/datasets/Adult). It consists of many multivariate features including demographics and income information. The task is to predict, if the income of a person is above or below $50k. \n",
    "\n",
    "### Setup\n",
    "\n",
    "To get started, make sure you have these prerequisites completed.\n",
    "\n",
    "* Specify an AWS Region to host your model.\n",
    "* An IAM role ARN exists that is used to give Amazon SageMaker access to your data in Amazon Simple Storage Service (Amazon S3). See the documentation for how to fine tune the permissions needed.\n",
    "* Create an S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations. For demonstration purposes, you are using the same bucket for these. In reality, you might want to separate them with different security policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import tarfile\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()  # this could also be a hard-coded bucket name\n",
    "\n",
    "print('Using bucket ' + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116de51d",
   "metadata": {},
   "source": [
    "### Download data from UCI repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "mv adult.data ./data/adult.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f560d",
   "metadata": {},
   "source": [
    "## Pre-process\n",
    "\n",
    "Create a Scikit-learn pipeline to handle pre-processing. It consists of following steps:\n",
    "* Create train-test split\n",
    "* Use simple imputer to substitute most frequent for categorical and mean for numerical features.\n",
    "* Use one-hot encoding for handling categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e504fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv with column names\n",
    "column_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
    "df = pd.read_csv('data/adult.csv', names = column_names)\n",
    "\n",
    "df.replace('?',np.NaN,inplace=True)\n",
    "\n",
    "df_train_val, df_test, = train_test_split(df, test_size=0.1, random_state=42)\n",
    "df_train_val_no_target = df_train_val.drop('income', axis=1)\n",
    "\n",
    "df_test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_ind = [i for i, x in enumerate(df_train_val_no_target.dtypes) if x != object]\n",
    "cat_ind = [i for i, x in enumerate(df_train_val_no_target.dtypes) if x == object]\n",
    "\n",
    "numeric_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_ind),\n",
    "    ('cat', categorical_transformer, cat_ind)\n",
    "])\n",
    "\n",
    "X = preprocessor.fit_transform(df_train_val_no_target)\n",
    "\n",
    "y = LabelEncoder().fit_transform(df_train_val.income)\n",
    "X = np.insert(X, 0, y, axis=1)\n",
    "\n",
    "# Save the ColumnTransformer to be used during inference\n",
    "with open('script/preprocess.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa51bd1",
   "metadata": {},
   "source": [
    "### Train-val split\n",
    "Split the training set again to create validation set and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312802f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "np.savetxt(\"data/train.csv\", X_train, delimiter=\",\", fmt='%f')\n",
    "np.savetxt(\"data/val.csv\", X_val, delimiter=\",\", fmt='%f')\n",
    "\n",
    "prefix = 'sagemaker/blog'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv'))\\\n",
    ".upload_file('data/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv'))\\\n",
    ".upload_file('data/val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3046c",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "We train the model using SageMaker built-in XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, '1.2-1')\n",
    "\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    get_execution_role(), \n",
    "                                    hyperparameters=hyperparameters,                                    \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to be used during inference\n",
    "!aws s3 cp {xgb.model_data} model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
